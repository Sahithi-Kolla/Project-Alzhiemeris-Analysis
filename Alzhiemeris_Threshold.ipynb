{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Required Packages for the Project RUN ONLY ONCE\n",
    "# %pip install matplotlib\n",
    "# %pip install nilearn\n",
    "# %pip install openpyxl\n",
    "# %pip install Path\n",
    "# %pip install seaborn\n",
    "# %pip install nltools\n",
    "# %pip install scipy\n",
    "# %pip install scikit-image\n",
    "# %pip install nibabel\n",
    "# %pip install bctpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# from skimage import filters\n",
    "# import json\n",
    "# import nibabel as nib\n",
    "# from nilearn.plotting import plot_img\n",
    "\n",
    "import pandas as pd\n",
    "from nilearn import image\n",
    "from nilearn.image import load_img, index_img\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import networkx as nx\n",
    "import matplotlib as mt\n",
    "import scipy as sc\n",
    "import seaborn as sns\n",
    "import bct\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectData:\n",
    "    \n",
    "    def __init__(self, datasetPath, componentsPath):\n",
    "        self.componentFileName = \"adni_aa__sub01_component_ica_s1_.nii\"\n",
    "        self.timeCourseFileName = \"adni_aa__sub01_timecourses_ica_s1_.nii\"\n",
    "        self.FNCmatFile = \"adni_aa__postprocess_results.mat\"\n",
    "        self.component_key = \"fnc_corrs_all\"\n",
    "        self.graphMetricGlobalMeasues = [\n",
    "                # Global Measures\n",
    "                \"Global efficiency\", \n",
    "                \"Characteristic path length\", \n",
    "                \"Clustering coefficient\",\n",
    "            ]\n",
    "        self.graphMetricComponentMeasures = [\n",
    "                # Component Measures\n",
    "                \"Degree\",\n",
    "                \"Closeness centrality\",\n",
    "                \"Participation coefficient\"\n",
    "            ]\n",
    "        self.subjectsData = self.readFileofCSV(datasetPath)\n",
    "        self.componentData = self.readFileofCSV(componentsPath)\n",
    "\n",
    "        self.modifySubjectsPath()\n",
    "\n",
    "        self.domainList = self.componentData[\"icn_domain\"]\n",
    "        self.indexList = self.componentData[\"icn_index\"]\n",
    "\n",
    "    def modifySubjectsPath(self):\n",
    "        for i in range(2404):\n",
    "            self.subjectsData.at[i,\"fc_dir\"] = self.subjectsData.iloc[i][\"fc_dir\"].replace(\"FC\",\"GIGICA\")\n",
    "\n",
    "    def readFileofCSV(self,path):\n",
    "        fileData = pd.read_csv(path)\n",
    "        return fileData\n",
    "    \n",
    "    def getDatasetPaths(self,subjectName):\n",
    "        ans = list()\n",
    "        for i in range(2404):\n",
    "            if self.subjectsData.at[i,\"ResearchGroup\"] == subjectName:\n",
    "                ans.append(self.subjectsData.at[i,\"fc_dir\"])\n",
    "        return ans\n",
    "\n",
    "    def calculatePearsonCrossCorrelations(self, listA, listB):\n",
    "        xmean = np.mean(listA)\n",
    "        xval = listA - xmean\n",
    "        xsqu = np.sqrt(np.sum(np.square(xval)))\n",
    "\n",
    "        ymean = np.mean(listB)\n",
    "        yval = listB - ymean\n",
    "        ysqu = np.sqrt(np.sum(np.square(yval)))\n",
    "\n",
    "        num = 0\n",
    "        for k in range(len(yval)):\n",
    "            num += (xval[k]*yval[k])\n",
    "\n",
    "        den = (xsqu * ysqu)\n",
    "        ans = num/den\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelCounts(SubjectData):\n",
    "    \" \\\n",
    "        datasetPath : 4D images path \\\n",
    "        componentsPath: indexes path \\\n",
    "        subjectList: List of subjects to calculate ex: AD, CN, ....  \\\n",
    "    \"\n",
    "    def __init__(self,datasetPath, componentsPath, subjectList):\n",
    "        super().__init__(datasetPath,componentsPath)\n",
    "        self.subjectsToCalculate = subjectList\n",
    "\n",
    "        self.voxelCountMap = dict()\n",
    "        self.prepareVoxelCountMap()\n",
    "\n",
    "    def prepareVoxelCountMap(self):\n",
    "        \" \\\n",
    "            iterate over the list of Subjects like AD, CN, ....     \\\n",
    "        \"\n",
    "        for subjectName in self.subjectsToCalculate:\n",
    "            self.voxelCountMap[subjectName]=dict()\n",
    "            self.voxelCountMap[subjectName][\"paths\"] = self.getDatasetPaths(subjectName)\n",
    "            self.voxelCountMap[subjectName][\"indexes\"] = dict()\n",
    "            for i in self.indexList:\n",
    "                self.voxelCountMap[subjectName][\"indexes\"][i]=list()\n",
    "\n",
    "    def calculateVoxelCount(self):\n",
    "        for subjectName in self.subjectsToCalculate:\n",
    "            for path in self.voxelCountMap[subjectName][\"paths\"]:\n",
    "                spacialMapName = path + self.componentFileName\n",
    "                spacialMap = load_img(spacialMapName)\n",
    "                for index in self.voxelCountMap[subjectName][\"indexes\"]:\n",
    "                    actualIndex=index-1\n",
    "                    componentImg = index_img(spacialMap, actualIndex)\n",
    "                    componentImgData = componentImg.get_fdata()\n",
    "                    component_threshold = 3*np.std(componentImgData)\n",
    "                    component_voxelCount = np.count_nonzero(componentImgData > component_threshold)\n",
    "                    self.voxelCountMap[subjectName][\"indexes\"][index].append(component_voxelCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AD_Threshold = VoxelCounts('ADNI_demos.txt', 'NM_icns_info.csv', [ \"CN\", \"MCI\",  \"AD\" ] )\n",
    "AD_Threshold.calculateVoxelCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMetrics(SubjectData):\n",
    "\n",
    "    \" \\\n",
    "        datasetPath : 4D images path \\\n",
    "        componentsPath: indexes path \\\n",
    "        subjectList: List of subjects to calculate ex: AD, CN, ....  \\\n",
    "    \"\n",
    "\n",
    "    def __init__(self,datasetPath, componentsPath, subjectList):\n",
    "        super().__init__(datasetPath,componentsPath)\n",
    "        self.subjectsToCalculate = subjectList\n",
    "\n",
    "        self.graphMetricMap = dict()\n",
    "        self.prepareGraphMetricMap()\n",
    "\n",
    "    def prepareGraphMetricMap(self):\n",
    "        for subjectName in self.subjectsToCalculate:\n",
    "            self.graphMetricMap[subjectName]=dict()\n",
    "            self.graphMetricMap[subjectName][\"paths\"] = self.getDatasetPaths(subjectName)\n",
    "            for graphMetric in self.graphMetricGlobalMeasues:\n",
    "                self.graphMetricMap[subjectName][graphMetric]=list()\n",
    "\n",
    "            for graphMetric in self.graphMetricComponentMeasures:\n",
    "                self.graphMetricMap[subjectName][graphMetric] = dict()\n",
    "                for ind in self.indexList:\n",
    "                    self.graphMetricMap[subjectName][graphMetric][ind]=list()\n",
    "\n",
    "    def loadMatFile(self,filePath):\n",
    "        componentDict = scipy.io.loadmat(filePath+self.FNCmatFile)\n",
    "        return componentDict[self.component_key]\n",
    "\n",
    "    def prepareFNCMatrix(self,componentData):\n",
    "        selected_component = np.zeros((53,53), dtype=np.float64).reshape(53,53)\n",
    "        for i in range(53):\n",
    "            for j in range(i+1, 53):\n",
    "                # if componentData[self.indexList[i]-1][self.indexList[j]-1] >=0 : \n",
    "                selected_component[i][j]=componentData[self.indexList[i]-1][self.indexList[j]-1]\n",
    "        selected_component += selected_component.T\n",
    "\n",
    "        # Finding correlation matrix\n",
    "        corrs = pd.DataFrame(selected_component)\n",
    "        correlation_matrix = corrs.corr()\n",
    "        correlation_numpy_array = correlation_matrix.to_numpy(dtype=np.float64)\n",
    "\n",
    "        # Finding Thresholded correlation matrix\n",
    "        row,column = correlation_numpy_array.shape\n",
    "        for i in range(row):\n",
    "            for j in range(column):\n",
    "                if i!=j and correlation_numpy_array[i][j]<0:\n",
    "                    correlation_numpy_array[i][j]=0\n",
    "\n",
    "        input_matrix = nx.from_numpy_array(correlation_numpy_array)\n",
    "        return input_matrix\n",
    "\n",
    "    def calculateGraphMetrics(self):\n",
    "        for subjectName in self.subjectsToCalculate:\n",
    "            for path in self.graphMetricMap[subjectName][\"paths\"]:\n",
    "                fncMatrix = self.prepareFNCMatrix(self.loadMatFile(path))\n",
    "\n",
    "                # for global measures:\n",
    "                for graphMetric in self.graphMetricGlobalMeasues:\n",
    "                    if graphMetric == \"Global efficiency\":\n",
    "                        globalEfficieny = nx.global_efficiency(fncMatrix)\n",
    "                        self.graphMetricMap[subjectName][graphMetric].append(globalEfficieny)\n",
    "\n",
    "                    elif graphMetric == \"Characteristic path length\":\n",
    "                        characteristicPathLength = nx.average_shortest_path_length(fncMatrix, weight='weight')\n",
    "                        self.graphMetricMap[subjectName][graphMetric].append(characteristicPathLength)\n",
    "\n",
    "                    elif graphMetric == \"Clustering coefficient\":\n",
    "                        clusterCofficient = nx.clustering(fncMatrix, weight='weight')\n",
    "                        clusterCofficient_network = 0\n",
    "                        for i in clusterCofficient:\n",
    "                            clusterCofficient_network += clusterCofficient[i]\n",
    "                        clusterCofficient_network /= 53\n",
    "                        self.graphMetricMap[subjectName][graphMetric].append(clusterCofficient_network)\n",
    "\n",
    "                    else:\n",
    "                        print(\"Not Found: \", graphMetric)\n",
    "                        assert(False)\n",
    "\n",
    "                # for component measures\n",
    "                for graphMetric in self.graphMetricComponentMeasures:\n",
    "                    if graphMetric == \"Degree\":\n",
    "                        all_nodes_degree = nx.degree(fncMatrix, weight= 'weight')\n",
    "                        for ind in range(len(self.indexList)):\n",
    "                            self.graphMetricMap[subjectName][graphMetric][self.indexList[ind]].append(all_nodes_degree[ind])\n",
    "\n",
    "                    elif graphMetric == \"Closeness centrality\":\n",
    "                        all_nodes_cc = nx.closeness_centrality(fncMatrix, distance='weight', wf_improved=False)\n",
    "                        for ind in range(len(self.indexList)):\n",
    "                            self.graphMetricMap[subjectName][graphMetric][self.indexList[ind]].append(all_nodes_cc[ind])\n",
    "\n",
    "                    elif graphMetric == \"Participation coefficient\":\n",
    "                        fncMatrix_numpy = nx.to_numpy_array(fncMatrix)\n",
    "                        modularity = nx.algorithms.community.greedy_modularity_communities(fncMatrix)\n",
    "                        sam=[]\n",
    "                        for i in range(len(modularity)):\n",
    "                            for j in list(modularity[i]):\n",
    "                                sam.append(j)\n",
    "                        sam = np.array(sam)\n",
    "                        all_nodes_pc = bct.centrality.participation_coef(fncMatrix_numpy, ci=sam, degree='undirected')\n",
    "                        for ind in range(len(self.indexList)):\n",
    "                            self.graphMetricMap[subjectName][graphMetric][self.indexList[ind]].append(all_nodes_pc[ind])\n",
    "\n",
    "                    else:\n",
    "                        print(\"Not Found: \", graphMetric)\n",
    "                        assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AD_GraphMetric = GraphMetrics('ADNI_demos.txt', 'NM_icns_info.csv', [ \"CN\", \"MCI\", \"AD\" ])\n",
    "AD_GraphMetric.calculateGraphMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Global specific measures.\n",
    "\n",
    "GlobalGraphMetrics_Names = AD_GraphMetric.graphMetricGlobalMeasues\n",
    "ComponentGraphMetric_Names = AD_GraphMetric.graphMetricComponentMeasures\n",
    "ans_global = list()\n",
    "ans_component = list()\n",
    "\n",
    "for subjectName in AD_GraphMetric.subjectsToCalculate:\n",
    "\n",
    "    table_global = pd.DataFrame()\n",
    "    table_component = pd.DataFrame()\n",
    "    for i in range(len(AD_Threshold.indexList)):\n",
    "        table_global[AD_Threshold.domainList[i]+'('+str(AD_Threshold.indexList[i])+')'] = list()\n",
    "        table_component[AD_Threshold.domainList[i]+'('+str(AD_Threshold.indexList[i])+')'] = list()\n",
    "\n",
    "    for graphMetricName in GlobalGraphMetrics_Names:\n",
    "        indexList = AD_Threshold.indexList\n",
    "        corr_list_global = list()\n",
    "\n",
    "        xlist_global = deepcopy(AD_GraphMetric.graphMetricMap[subjectName][graphMetricName])\n",
    "        for j in range(len(indexList)):\n",
    "\n",
    "            ylist_global = deepcopy(AD_Threshold.voxelCountMap[subjectName][\"indexes\"][AD_Threshold.indexList[j]])\n",
    "            corr_list_global.append( AD_Threshold.calculatePearsonCrossCorrelations(xlist_global, ylist_global) )\n",
    "\n",
    "        table_global.loc[len(table_global.index)] = corr_list_global\n",
    "\n",
    "\n",
    "    for i in ComponentGraphMetric_Names:\n",
    "        indexList = AD_Threshold.indexList\n",
    "        corr_list_component = list()\n",
    "\n",
    "        for j in range(len(indexList)):\n",
    " \n",
    "            xlist_component = deepcopy(AD_GraphMetric.graphMetricMap[subjectName][i][indexList[j]])\n",
    "            ylist_component = deepcopy(AD_Threshold.voxelCountMap[subjectName][\"indexes\"][AD_Threshold.indexList[j]])\n",
    "            corr_list_component.append(AD_Threshold.calculatePearsonCrossCorrelations(xlist_component, ylist_component))\n",
    "\n",
    "        table_component.loc[len(table_component.index)] = corr_list_component\n",
    "\n",
    "    table_global.index = AD_GraphMetric.graphMetricGlobalMeasues\n",
    "    table_component.index = AD_GraphMetric.graphMetricComponentMeasures\n",
    "    ans_global.append(table_global)\n",
    "    ans_component.append(table_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_global = deepcopy(ans_global)\n",
    "temp_component = deepcopy(ans_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symvalue = 0.5\n",
    "fileNames = ['Controls', 'Mild_Cognetive_Impairment', 'Alzheimers']\n",
    "\n",
    "for i in range(len(fileNames)):\n",
    "        # temp_global[i].to_csv(fileNames[i]+'_global.csv')\n",
    "\n",
    "        s1 = temp_global[i].to_numpy()\n",
    "        fig, ax = mt.pyplot.subplots(figsize=(54,15))\n",
    "        columnsName = temp_global[i].columns.values\n",
    "        rowsName = temp_global[i].index.values\n",
    "        sd = sns.heatmap(\n",
    "                        s1, cmap='coolwarm', square=True, ax=ax, linewidth='0.5', center=0, vmin= -1*symvalue, vmax=symvalue,\n",
    "                        xticklabels=columnsName, yticklabels=rowsName,  annot=True\n",
    "                )\n",
    "        ax.set_title(fileNames[i]+str(symvalue)+'_global.png')\n",
    "        sd.get_figure().savefig(fileNames[i]+str(symvalue)+'_global.png')\n",
    "\n",
    "for i in range(len(fileNames)):\n",
    "        # temp_component[i].to_csv(fileNames[i]+'_component.csv')\n",
    "\n",
    "        s1 = temp_component[i].to_numpy()\n",
    "        fig, ax = mt.pyplot.subplots(figsize=(54,15))\n",
    "        columnsName = temp_component[i].columns.values\n",
    "        rowsName = temp_component[i].index.values\n",
    "        sd = sns.heatmap(\n",
    "                        s1, cmap='coolwarm', square=True, ax=ax, linewidth='0.5', center=0, vmin= -1*symvalue, vmax=symvalue,\n",
    "                        xticklabels=columnsName, yticklabels=rowsName,  annot=True\n",
    "                )\n",
    "        ax.set_title(fileNames[i]+str(symvalue)+'_component.png')\n",
    "        sd.get_figure().savefig(fileNames[i]+str(symvalue)+'_component.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDifferenceCrossCorrelation(name1, name2, name3, tab1, tab2):\n",
    "    diff = pd.DataFrame().reindex_like(tab1).dropna()\n",
    "    columns = tab1.columns.values\n",
    "\n",
    "    rows = tab1.index.values\n",
    "\n",
    "    for i in rows:\n",
    "        for j in columns:\n",
    "            diff.at[i,j] = tab1.at[i,j] - tab2.at[i,j]\n",
    "\n",
    "    st1 = diff.to_numpy()\n",
    "\n",
    "    fig, ax = mt.pyplot.subplots(figsize=(54,15))\n",
    "\n",
    "    sd = sns.heatmap (\n",
    "                    st1, \n",
    "                    vmin= -1*symvalue, \n",
    "                    vmax= symvalue,\n",
    "                    cmap='coolwarm', \n",
    "                    square=True,\n",
    "                    ax=ax,\n",
    "                    linewidth='0.5',\n",
    "                    center=0, \n",
    "                    cbar=True,\n",
    "                    xticklabels=columnsName, \n",
    "                    yticklabels=rowsName,  \n",
    "                    annot=True\n",
    "                )\n",
    "    ax.set_title(name1+'-'+name2+'-'+name3+str(symvalue)+'.png')\n",
    "    sd.get_figure().savefig(name1+'-'+name2+'-'+name3+str(symvalue)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeN = len(fileNames)\n",
    "\n",
    "for i in range(sizeN):\n",
    "    for j in range(i+1,sizeN):\n",
    "        findDifferenceCrossCorrelation(fileNames[i], fileNames[j], 'global', temp_global[i], temp_global[j])\n",
    "        findDifferenceCrossCorrelation(fileNames[i], fileNames[j], 'component', temp_component[i], temp_component[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def concat_images(image_paths, size, shape=None):\n",
    "    # Open images and resize them\n",
    "    width, height = size\n",
    "    images = map(Image.open, image_paths)\n",
    "    images = [ImageOps.fit(image, size, Image.ANTIALIAS) \n",
    "              for image in images]\n",
    "    \n",
    "    # Create canvas for the final image with total size\n",
    "    shape = shape if shape else (1, len(images))\n",
    "    image_size = (width * shape[1], height * shape[0])\n",
    "    image = Image.new('RGB', image_size)\n",
    "    \n",
    "    # Paste images into final image\n",
    "    for row in range(shape[0]):\n",
    "        for col in range(shape[1]):\n",
    "            offset = width * col, height * row\n",
    "            idx = row * shape[1] + col\n",
    "            image.paste(images[idx], offset)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def createImage(keyword):\n",
    "    # Get list of image paths\n",
    "    folder = '.'\n",
    "    for key in keyword:\n",
    "        image_array = [ i+str(symvalue)+'_'+key+'.png' for i in fileNames ]\n",
    "        for i in range(sizeN):\n",
    "            for j in range(i+1,sizeN):\n",
    "                image_array.append(fileNames[i]+'-'+fileNames[j]+'-'+key+str(symvalue)+'.png')\n",
    "        image = concat_images(image_array, (3888,1080), (3,2))\n",
    "        image.save('image_'+key+'.png', 'PNG')\n",
    "\n",
    "createImage(['global', 'component'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1 4d subject\n",
    "    - 53 selected Components\n",
    "        - each component voxel Count - index i\n",
    "\n",
    "AD- 213\n",
    "69  - [ 1_Sub_69Ind_voxelCount, 2_Sub_69Ind_VoxelCount,  ]\n",
    "53  - [ ]\n",
    "\n",
    "1 4D subject:\n",
    "    - preprocess.mat file \n",
    "        - FNC matrix\n",
    "            - Graph metric \n",
    "                - Glo\n",
    "                - Charac\n",
    "                - Coefficint\n",
    "\n",
    "AD- 213\n",
    "Global Efficieny - [ 1_Sub_mat_FNC_Global_Value, 2_Sub_mat_FNC_Global, ..... ]\n",
    "Characterstic Path lenght - [ 1_sub, ........ ]\n",
    "\n",
    "\n",
    "Correlation:\n",
    "Global Efficiency Vs 69 [] - Correlation 0.123\n",
    "Global Efficiency Vs 53 [] - Correlation -0.324\n",
    "                                            0.372\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
